{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R4QGmXiq67E"
      },
      "source": [
        "### iTerms: Deconstructing ToS with LLMs\n",
        "Defining legal requirements as code enables users to automatically and more efficiently assess compliance with service restrictions. However, non-technical users face significant challenges when translating and understanding these clauses in existing formal languages. With iTerms, we propose a first approach that leverages LLMs as a bridge to translate Terms of Service clauses into the Terms of Service Language (TOSL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FS1T51Zq67F"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install langchain-openai\n",
        "! pip install langchain-community\n",
        "! pip install langchain-core\n",
        "! pip install pdfplumber\n",
        "! pip install rdflib requests\n",
        "! pip install -U \"openai>=1.46.0\" \"langchain>=0.2.12\" \"langchain-core>=0.2.34\" \"langchain-openai>=0.1.20\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0c22oWrq67F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "model = ChatOpenAI(model=\"gpt-5\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwbEqUN_ma1x"
      },
      "source": [
        "# Knowledge Graph Construction\n",
        "## Phase 1: Classification and Metadata Extraction\n",
        "\n",
        "In the first phase, each clause is analyzed to identify its nature and key semantic components.  \n",
        "\n",
        "- **Input:** use case to be analyzed and the clause in natural language  \n",
        "- **Processing:** classification of the clause and extraction of relevant metadata: `type`, `party`, `action`, and `asset`. These elements are aligned with the TOSL/ODRL vocabulary.  \n",
        "- **Output:** use case enriched with structured metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEFAULT_ACTIONS = [\n",
        "    'tosl:allowDownload','tosl:appeal','tosl:assign','tosl:claim','tosl:consent','tosl:develop','tosl:evaluate','tosl:integrate','tosl:procedure','tosl:publish','tosl:remove','tosl:terminate','tosl:test',\n",
        "    'odrl:acceptTracking','odrl:aggregate','odrl:anonymize','odrl:annotate','odrl:archive','odrl:attribute','odrl:compensate','odrl:concurrentUse','odrl:delete','odrl:derive','odrl:digitize','odrl:display','odrl:distribute','odrl:ensureExclusivity','odrl:execute','odrl:extract','odrl:give','odrl:grantUse','odrl:include','odrl:index','odrl:inform','odrl:install','odrl:modify','odrl:move','odrl:nextPolicy','odrl:obtainConsent','odrl:play','odrl:present','odrl:print','odrl:read','odrl:reproduce','odrl:reviewPolicy','odrl:sell','odrl:stream','odrl:synchronize','odrl:textToSpeech','odrl:transfer','odrl:transform','odrl:translate','odrl:uninstall','odrl:use','odrl:watermark',\n",
        "    'unspecified'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QESNO1dBnLHD"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a legal analyst specializing in SAAS Terms of Service.\n",
        "\n",
        "For the following text, perform these steps:\n",
        "\n",
        "1. Classify it EXCLUSIVELY as one of these 6 labels, in lowercase:\n",
        "    1. **permission**: When granting a right/authorization (e.g., \"may use\", \"right to access\", \"is permitted\").\n",
        "    2. **prohibition**: When restricting/forbidding actions (e.g., \"may not\", \"prohibited\", \"not allowed\").\n",
        "    3. **obligation**: When the party owes or is obliged to do something (e.g., \"must pay\", \"responsible for\", \"required to maintain\", \"will\").\n",
        "    4. **dispute resolution**: When establishing legal conflict mechanisms (e.g., \"arbitration\", \"jurisdiction\", \"governing law\").\n",
        "    5. **liability**: Responsibilities or limitations of liability for damages/warranties/indemnities (e.g., \"limitation of liability\", \"indirect damages\", \"warranty disclaimer\").\n",
        "    6. **other**: ONLY for:\n",
        "      - Pure titles/section headers (e.g., \"APPENDIX A: DEFINITIONS\")\n",
        "      - Non-substantive text (e.g., \"Effective Date: 2025-01-01\")\n",
        "      - Definition lists without operational context\n",
        "\n",
        "2. Identify the Party responsible for performing the action, this should be the \"Customer\" or the \"Provider\". If no Party is clearly specified, write \"unspecified\".\n",
        "\n",
        "3. From the list below, identify the Action that is most accurately permitted, obligatory, or prohibited within the clause. ONLY SELECT from the predefined list. Not create, modify, or assume new action names.\n",
        "    {actions_list}\n",
        "\n",
        "4. Identify the Object of the action (e.g.,“agreement”, “api service”, “service”, “api”, “dataset”, “data”, “user content”, “account”, “api key”, “documentation”, “sdk”, “website”, “model output”, “personal data”, “logs”, “application”, “software”, “credentials”, “billing info”, “other”). If no asset is clearly specified, write \"unspecified\".\n",
        "\n",
        "**Critical Instructions:**\n",
        "- Analyze ONLY substantive content (ignore formatting).\n",
        "- If the chunk contains both header and legal content, classify by content.\n",
        "- Use \"other\" ONLY for non-classifiable structural elements.\n",
        "\n",
        "**Response Format:**\n",
        "Return a JSON object with exactly these fields:\n",
        "\n",
        "{{\n",
        "  \"type\": \"<one of the 6 labels>\",\n",
        "  \"party\": \"<the party responsible>\",\n",
        "  \"action\": \"<the action permitted, obligatory or prohibited>\",\n",
        "  \"asset\": \"<the asset>\"\n",
        "}}\n",
        "\n",
        "Text to analyze:\n",
        "{clause}\n",
        "\"\"\")\n",
        "\n",
        "chat_model = ChatOpenAI(\n",
        "    model=\"gpt-5\",\n",
        "    temperature=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel,Field\n",
        "\n",
        "AllowedAction = Literal[\n",
        "    \"tosl:allowDownload\", \"tosl:appeal\", \"tosl:assign\", \"tosl:claim\", \"tosl:consent\",\n",
        "    \"tosl:develop\", \"tosl:evaluate\", \"tosl:integrate\", \"tosl:procedure\", \"tosl:publish\",\n",
        "    \"tosl:remove\", \"tosl:terminate\", \"tosl:test\",\n",
        "    \"odrl:acceptTracking\", \"odrl:aggregate\", \"odrl:anonymize\", \"odrl:annotate\", \"odrl:archive\",\n",
        "    \"odrl:attribute\", \"odrl:compensate\", \"odrl:concurrentUse\", \"odrl:delete\", \"odrl:derive\",\n",
        "    \"odrl:digitize\", \"odrl:display\", \"odrl:distribute\", \"odrl:ensureExclusivity\", \"odrl:execute\",\n",
        "    \"odrl:extract\", \"odrl:give\", \"odrl:grantUse\", \"odrl:include\", \"odrl:index\",\n",
        "    \"odrl:inform\", \"odrl:install\", \"odrl:modify\", \"odrl:move\", \"odrl:nextPolicy\",\n",
        "    \"odrl:obtainConsent\", \"odrl:play\", \"odrl:present\", \"odrl:print\", \"odrl:read\",\n",
        "    \"odrl:reproduce\", \"odrl:reviewPolicy\", \"odrl:sell\", \"odrl:stream\", \"odrl:synchronize\",\n",
        "    \"odrl:textToSpeech\", \"odrl:transfer\", \"odrl:transform\", \"odrl:translate\", \"odrl:uninstall\",\n",
        "    \"odrl:use\", \"odrl:watermark\",\n",
        "    \"unspecified\"\n",
        "]\n",
        "\n",
        "class ClauseResult(BaseModel):\n",
        "    type: Literal[\"permission\", \"prohibition\", \"obligation\", \"dispute resolution\", \"liability\", \"other\"] = Field(...)\n",
        "    party: Literal[\"customer\", \"provider\", \"unspecified\"] = Field(...)\n",
        "    action: AllowedAction = Field(...)\n",
        "    asset: str = Field(..., description=\"Short noun phrase (<= 3 words)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5775GNvnIsp"
      },
      "outputs": [],
      "source": [
        "structured_llm = chat_model.with_structured_output(ClauseResult)\n",
        "actions_text = \", \".join(f\"`{a}`\" for a in DEFAULT_ACTIONS) \n",
        "chain = prompt_template.partial(actions_list=actions_text) | structured_llm\n",
        "\n",
        "def classify_and_extract(clause: str) -> dict:\n",
        "    result: ClauseResult = chain.invoke({\"clause\": clause})\n",
        "    return result.model_dump()\n",
        "\n",
        "def process_use_cases(use_cases: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Itera sobre los use cases y añade los campos.\n",
        "    \"\"\"\n",
        "    for key, value in use_cases.items():\n",
        "        description = value.get(\"description\", \"\").strip()\n",
        "        if not description:\n",
        "            value[\"type\"] = \"other\"\n",
        "            value[\"party\"] = \"unspecified\"\n",
        "            value[\"action\"] = \"unspecified\"\n",
        "            value[\"asset\"] = \"unspecified\"\n",
        "            continue\n",
        "\n",
        "        result = classify_and_extract(description)\n",
        "        value[\"type\"] = (result.get(\"type\") or \"other\").strip().lower()\n",
        "        value[\"party\"] = (result.get(\"party\") or \"unspecified\").strip().lower()\n",
        "        value[\"action\"] = (result.get(\"action\") or \"unspecified\").strip().lower()\n",
        "        value[\"asset\"] = (result.get(\"asset\") or \"unspecified\").strip().lower()\n",
        "\n",
        "    return use_cases\n",
        "\n",
        "def main(input_file: str, output_file: str):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    use_cases = data.get(\"USE_CASE_DESCRIPTIONS\", {})\n",
        "    updated = process_use_cases(use_cases)\n",
        "    data[\"USE_CASE_DESCRIPTIONS\"] = updated\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eBKijNYmj4Y"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    INPUT_JSON = \"data/use_cases_elsevier.json\"\n",
        "    OUTPUT_JSON = \"phase1/results/use_cases_elsevier_classifieds.json\"\n",
        "\n",
        "    main(INPUT_JSON, OUTPUT_JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pathlib\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ASSET_ALIASES = {\n",
        "    \"api service's web site\": \"api service\",\n",
        "    \"api service’s web site\": \"api service\",\n",
        "    \"api services\": \"api service\",\n",
        "    \"api service access\": \"api service\",\n",
        "    \"api access\": \"api service\",\n",
        "    \"api keys\": \"api service\",\n",
        "    \"api key\": \"api service\",\n",
        "    \"elsevier content\": \"api service\"\n",
        "}\n",
        "\n",
        "FIELDS_EVAL = [\"type\", \"party\", \"action\", \"asset\"]\n",
        "\n",
        "def _load_uc(path: str):\n",
        "    d = json.loads(pathlib.Path(path).read_text(encoding=\"utf-8\"))\n",
        "    return d.get(\"USE_CASE_DESCRIPTIONS\") or d.get(\"use_cases\") or {}\n",
        "\n",
        "\n",
        "def _norm_type(x):\n",
        "    return (\"\" if x is None else str(x).strip().lower())\n",
        "\n",
        "\n",
        "def _norm_party(x, normalize_roles):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = str(x).strip()\n",
        "    return s if not normalize_roles else s.lower()\n",
        "\n",
        "\n",
        "def _norm_action(x, normalize_actions):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = str(x).strip()\n",
        "    return s if not normalize_actions else s\n",
        "\n",
        "\n",
        "def _norm_asset(x, normalize_assets):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = str(x).strip().lower()\n",
        "    return ASSET_ALIASES.get(s, s) if normalize_assets else s\n",
        "\n",
        "\n",
        "def _normalize_row(row, normalize_actions=False, normalize_roles=False, normalize_assets=True):\n",
        "    return {\n",
        "        \"type\":  _norm_type(row.get(\"type\")),\n",
        "        \"party\": _norm_party(row.get(\"party\"), normalize_roles),\n",
        "        \"action\": _norm_action(row.get(\"action\"), normalize_actions),\n",
        "        \"asset\": _norm_asset(row.get(\"asset\"), normalize_assets),\n",
        "    }\n",
        "\n",
        "\n",
        "def _confusion_and_metrics(y_true, y_pred):\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "    idx = {lab: i for i, lab in enumerate(labels)}\n",
        "    n = len(labels)\n",
        "    cm = [[0]*n for _ in range(n)]\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[idx[t]][idx[p]] += 1\n",
        "\n",
        "    tp_sum = fp_sum = fn_sum = 0\n",
        "    prec = rec = f1 = {}\n",
        "    support = {}\n",
        "    for i, lab in enumerate(labels):\n",
        "        tp = cm[i][i]\n",
        "        fp = sum(cm[r][i] for r in range(n) if r != i)\n",
        "        fn = sum(cm[i][c] for c in range(n) if c != i)\n",
        "        sup = sum(cm[i])\n",
        "        p = tp/(tp+fp) if (tp+fp) > 0 else 0.0\n",
        "        r = tp/(tp+fn) if (tp+fn) > 0 else 0.0\n",
        "        f = 2*p*r/(p+r) if (p+r) > 0 else 0.0\n",
        "        prec[lab], rec[lab], f1[lab], support[lab] = p, r, f, sup\n",
        "        tp_sum += tp\n",
        "        fp_sum += fp\n",
        "        fn_sum += fn\n",
        "    micro_p = tp_sum/(tp_sum+fp_sum) if (tp_sum+fp_sum) > 0 else 0.0\n",
        "    micro_r = tp_sum/(tp_sum+fn_sum) if (tp_sum+fn_sum) > 0 else 0.0\n",
        "    micro_f = 2*micro_p*micro_r / \\\n",
        "        (micro_p+micro_r) if (micro_p+micro_r) > 0 else 0.0\n",
        "    macro_p = sum(prec.values())/(len(labels) or 1)\n",
        "    macro_r = sum(rec.values())/(len(labels) or 1)\n",
        "    macro_f = sum(f1.values())/(len(labels) or 1)\n",
        "    total = sum(sum(r) for r in cm)\n",
        "    acc = sum(cm[i][i] for i in range(n))/(total or 1)\n",
        "    return {\n",
        "        \"labels\": labels, \"confusion\": cm, \"per_label\": {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"support\": support},\n",
        "        \"macro\": {\"precision\": macro_p, \"recall\": macro_r, \"f1\": macro_f},\n",
        "        \"micro\": {\"precision\": micro_p, \"recall\": micro_r, \"f1\": micro_f},\n",
        "        \"accuracy\": acc, \"total\": total,\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_phase1(pred_path, exp_path, out_json_path, normalize_actions=False, normalize_roles=False, normalize_assets=True):\n",
        "    pred_uc = _load_uc(pred_path)\n",
        "    exp_uc = _load_uc(exp_path)\n",
        "    keys_all = sorted(set(pred_uc.keys()) | set(exp_uc.keys()))\n",
        "    keys_eval = sorted(set(pred_uc.keys()) & set(exp_uc.keys()))\n",
        "\n",
        "    per_case = []\n",
        "    for k in keys_all:\n",
        "        p = _normalize_row(pred_uc.get(k, {}), normalize_actions,\n",
        "                           normalize_roles, normalize_assets)\n",
        "        e = _normalize_row(exp_uc.get(k, {}),  normalize_actions,\n",
        "                           normalize_roles, normalize_assets)\n",
        "        matches = {f: int(p.get(f) == e.get(f)) for f in FIELDS_EVAL}\n",
        "        per_case.append({\"id\": k, \"pred\": p, \"exp\": e, \"match\": matches})\n",
        "\n",
        "    per_field = {}\n",
        "    for f in FIELDS_EVAL:\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for k in keys_eval:\n",
        "            e = _normalize_row(exp_uc.get(k, {}),  normalize_actions,\n",
        "                               normalize_roles, normalize_assets).get(f, \"\")\n",
        "            p = _normalize_row(pred_uc.get(k, {}), normalize_actions,\n",
        "                               normalize_roles, normalize_assets).get(f, \"\")\n",
        "            y_true.append(\"\" if e is None else str(e))\n",
        "            y_pred.append(\"\" if p is None else str(p))\n",
        "        per_field[f] = _confusion_and_metrics(y_true, y_pred)\n",
        "\n",
        "    result = {\n",
        "        \"meta\": {\n",
        "            \"pred_path\": pred_path, \"exp_path\": exp_path,\n",
        "            \"n_cases_all\": len(keys_all), \"n_cases_eval\": len(keys_eval),\n",
        "            \"normalize_actions\": normalize_actions, \"normalize_roles\": normalize_roles,\n",
        "            \"normalize_assets\": normalize_assets, \"ignored\": [\"description\"],\n",
        "            \"timestamp\": int(time.time()),\n",
        "        },\n",
        "        \"per_case\": per_case,\n",
        "        \"per_field\": per_field,\n",
        "    }\n",
        "    out_p = pathlib.Path(out_json_path)\n",
        "    out_p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_p.write_text(json.dumps(result, ensure_ascii=False,\n",
        "                     indent=2), encoding=\"utf-8\")\n",
        "    print(f\"✓ Guardado JSON de métricas en {out_p}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _load_results(result_or_path):\n",
        "    return json.loads(pathlib.Path(result_or_path).read_text(encoding=\"utf-8\")) \\\n",
        "           if isinstance(result_or_path, (str, pathlib.Path)) else result_or_path\n",
        "\n",
        "def _weighted_f1_for_field(res, field):\n",
        "    m = res[\"per_field\"][field]\n",
        "    labels = m[\"labels\"]\n",
        "    sup = m[\"per_label\"][\"support\"]\n",
        "    f1s = m[\"per_label\"][\"f1\"]\n",
        "    total = sum(sup.values()) or 1\n",
        "    return sum((f1s.get(l, 0.0) * sup.get(l, 0)) for l in labels) / total\n",
        "\n",
        "def plot_phase1_metrics(result_or_path, save_path: str = None, show_weighted=True):\n",
        "    res = _load_results(result_or_path)\n",
        "    fields = FIELDS_EVAL  # [\"type\",\"party\",\"action\",\"asset\"]\n",
        "\n",
        "    accs = [res[\"per_field\"][f][\"accuracy\"] for f in fields]\n",
        "    f1m  = [res[\"per_field\"][f][\"macro\"][\"f1\"] for f in fields]\n",
        "    f1w  = [_weighted_f1_for_field(res, f) for f in fields] if show_weighted else None\n",
        "\n",
        "    # Pastel palette (fallback if seaborn not available)\n",
        "    pastel = ['#a1c9f4', '#ffb482', '#8de5a1']  # blue, orange, green (soft)\n",
        "    plt.figure(figsize=(9, 4))\n",
        "    x = range(len(fields)); w = 0.28\n",
        "\n",
        "    bars1 = plt.bar([i - (w if show_weighted else w/2) for i in x], accs, width=w, label=\"Accuracy\", color=pastel[0])\n",
        "    bars2 = plt.bar([i + (0 if show_weighted else w/2) for i in x], f1m, width=w, label=\"Macro-F1\", color=pastel[1])\n",
        "    if show_weighted:\n",
        "        bars3 = plt.bar([i + w for i in x], f1w, width=w, label=\"Weighted-F1\", color=pastel[2])\n",
        "\n",
        "    plt.xticks(list(x), [f.capitalize() for f in fields])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis=\"y\", alpha=0.25)\n",
        "    plt.title(\"Phase 1 — Accuracy vs Macro-F1\" + (\" (Weighted-F1 shown)\" if show_weighted else \"\"))\n",
        "    plt.legend()\n",
        "\n",
        "    def annotate(bars):\n",
        "        for b in bars:\n",
        "            h = b.get_height()\n",
        "            plt.text(b.get_x() + b.get_width()/2, h + 0.02, f\"{h:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    annotate(bars1); annotate(bars2)\n",
        "    if show_weighted:\n",
        "        annotate(bars3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        pathlib.Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=160)\n",
        "        print(f\"Saved chart to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_phase1_confusion(result_or_path, field: str = \"action\", save_path: str = None):\n",
        "    import json, pathlib\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if field not in FIELDS_EVAL:\n",
        "        raise ValueError(f\"field must be one of {FIELDS_EVAL}\")\n",
        "\n",
        "    res = json.loads(pathlib.Path(result_or_path).read_text(encoding=\"utf-8\")) \\\n",
        "          if isinstance(result_or_path, (str, pathlib.Path)) else result_or_path\n",
        "\n",
        "    m = res[\"per_field\"][field]\n",
        "    labels, cm = m[\"labels\"], m[\"confusion\"]\n",
        "\n",
        "    plt.figure(figsize=(max(6, 0.6*len(labels)), max(4, 0.6*len(labels))))\n",
        "    plt.imshow(cm, cmap=\"Blues\")\n",
        "    plt.colorbar(fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(labels)), labels)\n",
        "    plt.title(f\"Confusion Matrix — {field}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        for j in range(len(labels)):\n",
        "            v = cm[i][j]\n",
        "            if v:\n",
        "                plt.text(j, i, str(v), ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        pathlib.Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=160)\n",
        "        print(f\"✓ Saved confusion matrix to {save_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = compare_phase1(\n",
        "    pred_path=\"phase1/results/use_cases_elsevier_classifieds.json\",\n",
        "    exp_path=\"data/use_cases_elsevier_classifieds_expected.json\",\n",
        "    out_json_path=\"phase1/eval/elsevier_metadata_results.json\",\n",
        "    normalize_actions=False, normalize_roles=False, normalize_assets=True)\n",
        "\n",
        "plot_phase1_metrics(res, save_path=\"phase1/eval/metrics.png\", show_weighted=True)\n",
        "\n",
        "plot_phase1_confusion(res, field=\"action\", save_path=\"phase1/eval/confusion.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJnQ4anZPB"
      },
      "source": [
        "-------\n",
        "\n",
        "## Phase 2: Turtle Generation Guided by Template and Ontology\n",
        "\n",
        "The second phase focuses on transforming the enriched representation into an interoperable semantic format.  \n",
        "\n",
        "- **Inputs:** enriched use case, a Markdown-based template serving as a construction guide, and the TOSL/ODRL ontology as a semantic reference  \n",
        "- **Processing:** application of the template and the metamodel to generate RDF statements in Turtle syntax  \n",
        "- **Output:** Turtle file corresponding to the processed clause  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1XS-t2Nv3b3"
      },
      "outputs": [],
      "source": [
        "def load_use_cases(json_path):\n",
        "    \"\"\"\n",
        "    Carga el JSON y devuelve una lista de diccionarios con los campos relevantes.\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    global_metadata = {\n",
        "        \"PROVIDER\": data.get(\"PROVIDER\", \"Unknown\"),\n",
        "        \"SOURCE\": data.get(\"SOURCE\", \"Unknown\"),\n",
        "        \"TITLE\": data.get(\"TITLE\", \"Unknown\"),\n",
        "        \"DATE\": data.get(\"DATE\", \"Unknown\")\n",
        "    }\n",
        "\n",
        "    use_cases = []\n",
        "    for key, item in data.get(\"USE_CASE_DESCRIPTIONS\", {}).items():\n",
        "        clause = item.get(\"description\", \"\").strip()\n",
        "        classification = item.get(\"type\", \"\").strip().lower()\n",
        "\n",
        "        # Si Party o Asset no están, usa valores por defecto\n",
        "        party = item.get(\"party\", \"UnknownParty\")\n",
        "        action = item.get(\"action\", \"UnknownAction\")\n",
        "        asset = item.get(\"asset\", \"UnknownAsset\")\n",
        "\n",
        "        use_cases.append({\n",
        "            \"id\": key,\n",
        "            \"clause\": clause,\n",
        "            \"type\": classification,\n",
        "            \"party\": party,\n",
        "            \"action\": action,\n",
        "            \"asset\": asset\n",
        "        })\n",
        "\n",
        "    return use_cases, global_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cBXR3Cn3VnK"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_md(md_path):\n",
        "  with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      content = f.read()\n",
        "  return content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1ncBms5xHLF"
      },
      "outputs": [],
      "source": [
        "def extract_ontology(ontology_path):\n",
        "    with open(ontology_path, \"r\") as f:\n",
        "        content = f.read()\n",
        "        return content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uADMrnikDZNf"
      },
      "outputs": [],
      "source": [
        "def extract_ttl(content):\n",
        "    start_tag = \"```ttl\"\n",
        "    end_tag = \"```\"\n",
        "    start_idx = content.find(start_tag)\n",
        "    end_idx = content.rfind(end_tag)\n",
        "\n",
        "    if start_idx == -1 or end_idx == -1:\n",
        "        raise ValueError(\"No se encontraron los delimitadores ```ttl\")\n",
        "\n",
        "    # Extraer solo la parte del TTL\n",
        "    ttl = content[start_idx + len(start_tag):end_idx].strip()\n",
        "    return ttl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is8hJwBZGmzH"
      },
      "outputs": [],
      "source": [
        "def save_prompt(prompt_text, uc_id):\n",
        "    path = f\"phase2/results/prompt-{uc_id}.txt\"\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(prompt_text)\n",
        "\n",
        "def save_ttl(ttl_text, uc_id):\n",
        "    path = f\"phase2/results/{uc_id}.ttl\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(ttl_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1QPFZwOwEzS"
      },
      "outputs": [],
      "source": [
        "def build_prompt(ontology_context, template_text, clause_info, global_metadata):\n",
        "    \"\"\"\n",
        "    Construye un prompt de modelado RDF en Turtle.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "        You are a legal knowledge extractor specialized in TOSL and ODRL policy modeling.\n",
        "\n",
        "        Your job is to:\n",
        "        - Read a legal clause written in natural language.\n",
        "        - Identify the implied permissions, prohibitions, duties, limitations of liability, dispute resolution, and constraints.\n",
        "        - Convert the clause into a structured RDF policy using the ODRL and TOSL ontologies.\n",
        "\n",
        "        Guidelines:\n",
        "        - Output MUST be valid Turtle syntax.\n",
        "        - Use only the vocabulary and structure provided in the ontology context and reference template in markdown.\n",
        "        - All URIs and prefixes must be exactly as shown.\n",
        "        - Do not add explanations, headers, or any commentary.\n",
        "        - Output MUST be enclosed in a fenced code block with ```ttl.\n",
        "\n",
        "        Reference template: {template_text}\n",
        "\n",
        "        Metadata:\n",
        "          - Provider: {global_metadata.get(\"PROVIDER\", \"Unknown\")}\n",
        "          - Source: {global_metadata.get(\"SOURCE\", \"Unknown\")}\n",
        "          - Title: {global_metadata.get(\"TITLE\", \"Unknown\")}\n",
        "          - Date: {global_metadata.get(\"DATE\", \"Unknown\")}\n",
        "\n",
        "        Clause to model: {clause_info['clause']}\n",
        "\n",
        "        Classification type: {clause_info['type']}\n",
        "\n",
        "        Party: {clause_info['party']}\n",
        "\n",
        "        Suggested Action: {clause_info['action']}\n",
        "\n",
        "        Target asset: {clause_info['asset']}\n",
        "        \"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "def main(json_path, md_path, ontology_path):\n",
        "    \"\"\"\n",
        "    Orquesta el flujo:\n",
        "    1. Lee JSON.\n",
        "    2. Lee template markdown.\n",
        "    3. Construye prompts para cada cláusula.\n",
        "    \"\"\"\n",
        "    use_cases, global_metadata = load_use_cases(json_path)\n",
        "    template_text = extract_text_from_md(md_path)\n",
        "    os.makedirs(\"phase2/results\", exist_ok=True)\n",
        "\n",
        "    # Crear y mostrar prompts\n",
        "    for uc in use_cases:\n",
        "      prompt_text = build_prompt(template_text, uc, global_metadata)\n",
        "      save_prompt(prompt_text, uc['id'])\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are a legal knowledge extractor specialized in TOSL and ODRL policy modeling.\"},\n",
        "          {\"role\": \"user\", \"content\": prompt_text}\n",
        "      ]\n",
        "      response = model.invoke(messages)\n",
        "\n",
        "      print(\"======================================\")\n",
        "      print(f\"{uc['id']}\")\n",
        "      print(\"======================================\")\n",
        "      print(response.content)\n",
        "      ttl_text = extract_ttl(response.content)\n",
        "      save_ttl(ttl_text, uc['id'])\n",
        "      print(f\"✅ Archivo guardado\")\n",
        "      print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb4Gaz0Pwv2V",
        "outputId": "91293250-9dd8-43d3-e941-5406e306b405"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Paths de tus archivos\n",
        "    json_path = \"phase1/results/use_cases_elsevier_classifieds.json\"\n",
        "    md_path = \"template.md\"\n",
        "\n",
        "    main(json_path, md_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "PREFIXES = {\n",
        "    \"http://www.w3.org/ns/shacl#\": \"sh:\",\n",
        "    \"http://www.w3.org/ns/odrl/2/\": \"odrl:\",\n",
        "    \"https://w3id.org/tosl/\": \"tosl:\",\n",
        "    \"http://example.com/\": \":\",\n",
        "    \"http://www.w3.org/2001/XMLSchema#\": \"xsd:\",\n",
        "}\n",
        "def short(u: str) -> str:\n",
        "    if not u:\n",
        "        return \"\"\n",
        "    for base, pfx in PREFIXES.items():\n",
        "        if u.startswith(base):\n",
        "            return pfx + u[len(base):]\n",
        "    return u\n",
        "\n",
        "def tosl_checker(url, file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        r = requests.post(url, files={\"file\": (\"use_case.ttl\", f, \"text/turtle\")})\n",
        "\n",
        "    print(\"Código de estado:\", r.status_code)\n",
        "    try:\n",
        "        data = r.json()\n",
        "    except Exception:\n",
        "        print(\"Respuesta no-JSON:\\n\", r.text)\n",
        "        return None\n",
        "\n",
        "    ok = bool(data.get(\"conforms\") or data.get(\"valid\"))\n",
        "    print(\"✅ Conforms\" if ok else \"❌ Violations\")\n",
        "\n",
        "    if ok:\n",
        "        return data\n",
        "\n",
        "    for i, v in enumerate(data.get(\"violations\", []), 1):\n",
        "        focus = short(v.get(\"focusNode\", \"\"))\n",
        "        path  = short(v.get(\"resultPath\", \"\"))\n",
        "        value = short(v.get(\"valueNode\", \"\"))\n",
        "        msg   = v.get(\"message\", \"\")\n",
        "        head = f\"{focus} --{path}--> {value}\" if path else focus or value or \"(sin nodo)\"\n",
        "        print(f\"  {i}. {head}\")\n",
        "        if msg:\n",
        "            print(f\"      {msg}\")\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "VALIDATOR_URL = \"https://tosl.onrender.com/validator/validate\"\n",
        "\n",
        "def phase2_validate_and_plot(\n",
        "    url: str = VALIDATOR_URL,\n",
        "    in_globs = (\"phase2/results/*.ttl\",),\n",
        "    out_json: str = \"phase2/eval/elsevier_ttls_results.json\",\n",
        "    out_plot: str = \"phase2/eval/elsevier_ttls_results_plot.png\",\n",
        "    show_hist: bool = True,\n",
        "):\n",
        "    # 1) Collect files\n",
        "    paths = []\n",
        "    for g in in_globs:\n",
        "        paths.extend(pathlib.Path(\".\").glob(g))\n",
        "    paths = sorted(set(paths), key=lambda p: str(p))\n",
        "\n",
        "    # 2) Ensure output dir exists\n",
        "    out_path = pathlib.Path(out_json)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    pathlib.Path(out_plot).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 3) Validate with your tosl_checker and keep only {passed, errors}\n",
        "    results = {}\n",
        "    for p in paths:\n",
        "        print(f\"\\n[i] {p}\")\n",
        "        report = tosl_checker(url, str(p)) or {}\n",
        "        passed = bool(report.get(\"conforms\") or report.get(\"valid\"))\n",
        "        errors = len(report.get(\"violations\", [])) if isinstance(report, dict) else None\n",
        "        results[str(p)] = {\"passed\": passed, \"errors\": errors}\n",
        "\n",
        "    out_path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"\\nSaved JSON: {out_path} ({len(results)} items)\")\n",
        "\n",
        "    # 4) Plot and save\n",
        "    vals = list(results.values())\n",
        "    passed_n = sum(1 for v in vals if v.get(\"passed\"))\n",
        "    failed_n = len(vals) - passed_n\n",
        "    pass_rate = (passed_n / (passed_n + failed_n) * 100) if (passed_n + failed_n) else 0.0\n",
        "\n",
        "    c_pass, c_fail, c_hist = \"#8de5a1\", \"#ffb482\", \"#a1c9f4\"\n",
        "\n",
        "    if show_hist:\n",
        "        fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    else:\n",
        "        fig, ax0 = plt.subplots(1, 1, figsize=(7, 4))\n",
        "\n",
        "    # Bars: Passed/Failed\n",
        "    ax0.bar([\"Passed\", \"Failed\"], [passed_n, failed_n], color=[c_pass, c_fail])\n",
        "    for i, v in enumerate([passed_n, failed_n]):\n",
        "        ax0.text(i, v + max(1, v) * 0.02, f\"{v}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "    ax0.set_ylim(0, max(1, passed_n, failed_n) * 1.2)\n",
        "    ax0.set_title(f\"Phase 2 — Pass/Fail (Pass rate {pass_rate:.0f}%)\")\n",
        "    ax0.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "    # Histogram: violations per failed file\n",
        "    if show_hist:\n",
        "        errors = [int(v.get(\"errors\") or 0) for v in vals if not v.get(\"passed\")]\n",
        "        if errors:\n",
        "            bins = range(0, max(errors) + 2)\n",
        "            ax1.hist(errors, bins=bins, color=c_hist, edgecolor=\"white\")\n",
        "            ax1.set_title(\"Violations per failed file\")\n",
        "            ax1.set_xlabel(\"Violations\")\n",
        "            ax1.set_ylabel(\"Files\")\n",
        "            ax1.grid(axis=\"y\", alpha=0.3)\n",
        "        else:\n",
        "            ax1.axis(\"off\")\n",
        "            ax1.text(0.5, 0.5, \"No failed files\", ha=\"center\", va=\"center\", fontsize=11)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_plot, dpi=160)\n",
        "    print(f\"Saved plot: {out_plot}\")\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    phase2_validate_and_plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovh82uiHUs-"
      },
      "source": [
        "-------\n",
        "\n",
        "## Phase 3: Validation and Self-repair\n",
        "This phase ensures the syntactic and semantic correctness of the output.  \n",
        "\n",
        "- **Input:** Turtle file generated in the previous phase  \n",
        "- **Processing:**  \n",
        "  - Syntactic validation using standard Turtle validation tools  \n",
        "  - Semantic validation using a custom validator based on TOSL/ODRL rules  \n",
        "  - **AI-assisted self-repair** to automatically correct inconsistencies  \n",
        "- **Outputs:**  \n",
        "  1. Error report  \n",
        "  2. Corrected and validated Turtle file  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt_fix_ttl(ontology_context, instance_ttl, validator_errors_json):\n",
        "    \"\"\"\n",
        "    Builds a prompt to FIX a Turtle file using an ontology and validator feedback.\n",
        "    Output is requested as a fenced ```ttl code block.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "        You are a knowledge engineer specialized in RDF/Turtle and ontology-driven validation.\n",
        "\n",
        "        Your job is to:\n",
        "        - Read the original Turtle file and the validator violations.\n",
        "        - Fix ALL violations so that the file conforms to the given ontology.\n",
        "        - Preserve all correct information and the original meaning whenever possible.\n",
        "\n",
        "        Guidelines:\n",
        "        - Output MUST be valid Turtle syntax.\n",
        "        - Use ONLY the vocabulary and structure provided in the ontology context.\n",
        "        - Keep existing prefixes; add missing ones if needed (and only if required by the ontology).\n",
        "        - Do NOT invent IRIs if they can be derived from the ontology/prefixes.\n",
        "        - The output MUST contain ONLY the corrected TTL, enclosed in a fenced code block with ```ttl (no extra text).\n",
        "\n",
        "        Ontology context (.ttl):\n",
        "        {ontology_context}\n",
        "\n",
        "        Validator errors (JSON):\n",
        "        {validator_errors_json}\n",
        "\n",
        "        Original file (Turtle):\n",
        "        {instance_ttl}\n",
        "\n",
        "        IMPORTANT:\n",
        "        - Think step by step internally, but return ONLY the corrected Turtle in a ```ttl fenced block.\n",
        "    \"\"\"\n",
        "    return prompt.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "URL = \"https://tosl.onrender.com/validator/validate\"\n",
        "IN_DIR = Path(\"phase2/results\")\n",
        "OUT_DIR = Path(\"phase3/results\")\n",
        "EVAL_JSON = Path(\"phase3/eval/elsevier_ttls_results.json\")  \n",
        "ONTOLOGY_PATH = Path(\"tosl_odrl_simplified.ttl\")\n",
        "PATTERN = \"use_case_*.ttl\"\n",
        "\n",
        "def conforms(report: dict) -> bool:\n",
        "    return bool(report.get(\"conforms\") or report.get(\"valid\"))\n",
        "\n",
        "def _violations_count(report) -> int:\n",
        "    if isinstance(report, dict) and isinstance(report.get(\"violations\"), list):\n",
        "        return len(report[\"violations\"])\n",
        "    return 0\n",
        "\n",
        "def _load_results(path: Path) -> dict:\n",
        "    if path.exists():\n",
        "        try:\n",
        "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {}\n",
        "\n",
        "def _save_results(path: Path, results: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    path.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def main():\n",
        "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    ontology_context = extract_ontology(str(ONTOLOGY_PATH))\n",
        "    EVAL_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    results = _load_results(EVAL_JSON)\n",
        "\n",
        "    for f in sorted(IN_DIR.glob(PATTERN)):\n",
        "        key = str(f) \n",
        "        print(f\"[i] {f.name}\")\n",
        "        report = tosl_checker(URL, str(f))\n",
        "\n",
        "        if conforms(report):\n",
        "            out = OUT_DIR / f.name\n",
        "            out.write_text(f.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
        "            continue\n",
        "\n",
        "        prompt_text = build_prompt_fix_ttl(ontology_context=ontology_context, \n",
        "                                           instance_ttl=f.read_text(encoding=\"utf-8\"), \n",
        "                                           validator_errors_json=report.get(\"violations\", []))\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a strict RDF/Turtle engineer.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "        ]\n",
        "        response = model.invoke(messages)\n",
        "\n",
        "        ttl_text = extract_ttl(response.content)\n",
        "        out = OUT_DIR / f\"{f.stem}{f.suffix}\"\n",
        "        out.write_text(ttl_text, encoding=\"utf-8\")\n",
        "\n",
        "        # check again\n",
        "        reval = tosl_checker(URL, str(out))\n",
        "        ok = conforms(reval)\n",
        "        errs = 0 if ok else _violations_count(reval)\n",
        "        results[key] = {\"passed\": ok, \"errors\": errs}\n",
        "        _save_results(EVAL_JSON, results)\n",
        "        print(\"   🔧 Save as:\", out.name, \"|\", \"✅ Conforms\" if conforms(reval) else \"❌ No conforms\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------\n",
        "\n",
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, time, pathlib, concurrent.futures as cf\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "DEONTIC_URL = \"https://tosl.onrender.com/sparql/deontic_status\"\n",
        "UNFAIR_URL  = \"https://tosl.onrender.com/sparql/unfair_terms\"\n",
        "\n",
        "DEONTIC_KEEP = [\"get_duties\", \"get_permissions\", \"get_prohibitions\", \"total_rules\"]\n",
        "\n",
        "TERM_TYPES = [\n",
        "    \"change\",\n",
        "    \"termination\",\n",
        "    \"contract_by_use\",\n",
        "    \"choice_of_law\",\n",
        "    \"jurisdiction\",\n",
        "    \"arbitration\",\n",
        "    \"content_removal\",\n",
        "    \"limitation_of_liability\",\n",
        "]\n",
        "\n",
        "SHORTEN = True\n",
        "PREFIXES = {\n",
        "    \"http://www.w3.org/ns/odrl/2/\": \"odrl:\",\n",
        "    \"https://w3id.org/tosl/\": \"tosl:\",\n",
        "    \"http://purl.org/dc/terms/\": \"dcterms:\",\n",
        "    \"http://example.com/\": \":\",\n",
        "    \"http://www.w3.org/2001/XMLSchema#\": \"xsd:\",\n",
        "}\n",
        "\n",
        "S = requests.Session()\n",
        "S.headers.update({\"Accept\": \"application/sparql-results+json\", \"Accept-Encoding\": \"gzip, deflate\"})\n",
        "S.mount(\"https://\", HTTPAdapter(max_retries=Retry(\n",
        "    total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=[\"POST\"]\n",
        ")))\n",
        "TIMEOUT = 60\n",
        "\n",
        "def short(u: str):\n",
        "    if not SHORTEN or not isinstance(u, str):\n",
        "        return u\n",
        "    for base, pfx in PREFIXES.items():\n",
        "        if u.startswith(base):\n",
        "            return pfx + u[len(base):]\n",
        "    return u\n",
        "\n",
        "def to_rows(payload):\n",
        "    if isinstance(payload, list):\n",
        "        return payload\n",
        "    if isinstance(payload, dict) and \"results\" in payload:\n",
        "        vars_ = payload.get(\"head\", {}).get(\"vars\", [])\n",
        "        return [{v: b.get(v, {}).get(\"value\") for v in payload[\"results\"].get(\"bindings\", [])}\n",
        "                for b in payload[\"results\"].get(\"bindings\", [])]\n",
        "    return []\n",
        "\n",
        "def as_list(x):\n",
        "    if x is None or x == \"\":\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return [short(v) for v in x]\n",
        "    return [short(v.strip()) for v in str(x).split(\",\") if v.strip()]\n",
        "\n",
        "def norm_deontic_row(row):\n",
        "    rid = row.get(\"duty\") or row.get(\"permission\") or row.get(\"prohibition\") or row.get(\"rule\") or row.get(\"element\")\n",
        "    return {\n",
        "        \"id\": short(rid),\n",
        "        \"actions\": as_list(row.get(\"actions\") or row.get(\"action\")),\n",
        "        \"targets\": as_list(row.get(\"targets\") or row.get(\"target\")),\n",
        "        \"assignee\": short(row.get(\"assignee\")),\n",
        "        \"assigner\": short(row.get(\"assigner\")),\n",
        "        \"description\": row.get(\"description\"),\n",
        "    }\n",
        "\n",
        "def post_file(url, ttl_path, params=None):\n",
        "    t0 = time.time()\n",
        "    ttl_path = pathlib.Path(ttl_path)\n",
        "    with open(ttl_path, \"rb\") as f:\n",
        "        r = S.post(url, params=params or {}, files={\"file\": (ttl_path.name, f, \"text/turtle\")}, timeout=TIMEOUT)\n",
        "    ms = int((time.time() - t0) * 1000)\n",
        "    try:\n",
        "        data, ok_json = r.json(), True\n",
        "    except Exception:\n",
        "        data, ok_json = {\"non_json\": r.text[:4000]}, False\n",
        "    return {\"status\": r.status_code, \"ms\": ms, \"ok\": r.ok and ok_json, \"data\": data}\n",
        "\n",
        "def run_all_for_file(ttl_path: str):\n",
        "    ttl_path = str(ttl_path)\n",
        "    out = {\n",
        "        \"file\": ttl_path,\n",
        "        \"deontic\": {\n",
        "            \"total_rules\": 0,\n",
        "            \"duties\": [],\n",
        "            \"permissions\": [],\n",
        "            \"prohibitions\": [],\n",
        "        },\n",
        "        \"unfair_terms\": {},\n",
        "        \"_meta\": {\"calls\": 0, \"ok\": 0, \"ms_total\": 0},  # opcional\n",
        "    }\n",
        "\n",
        "    for q in DEONTIC_KEEP:\n",
        "        res = post_file(DEONTIC_URL, ttl_path, params={\"deontic_status\": q})\n",
        "        out[\"_meta\"][\"calls\"] += 1\n",
        "        out[\"_meta\"][\"ok\"] += int(bool(res[\"ok\"]))\n",
        "        out[\"_meta\"][\"ms_total\"] += res[\"ms\"]\n",
        "        rows = to_rows(res[\"data\"]) if res[\"ok\"] else []\n",
        "        if q == \"total_rules\":\n",
        "            val = 0\n",
        "            if rows:\n",
        "                raw = rows[0].get(\"totalElements\")\n",
        "                try: val = int(raw or 0)\n",
        "                except Exception: val = 0\n",
        "            out[\"deontic\"][\"total_rules\"] = val\n",
        "        elif q == \"get_duties\":\n",
        "            out[\"deontic\"][\"duties\"] = [norm_deontic_row(r) for r in rows]\n",
        "        elif q == \"get_permissions\":\n",
        "            out[\"deontic\"][\"permissions\"] = [norm_deontic_row(r) for r in rows]\n",
        "        elif q == \"get_prohibitions\":\n",
        "            out[\"deontic\"][\"prohibitions\"] = [norm_deontic_row(r) for r in rows]\n",
        "\n",
        "    for tt in TERM_TYPES:\n",
        "        res = post_file(UNFAIR_URL, ttl_path, params={\"term_type\": tt})\n",
        "        out[\"_meta\"][\"calls\"] += 1\n",
        "        out[\"_meta\"][\"ok\"] += int(bool(res[\"ok\"]))\n",
        "        out[\"_meta\"][\"ms_total\"] += res[\"ms\"]\n",
        "        rows = to_rows(res[\"data\"]) if res[\"ok\"] else []\n",
        "        clean = []\n",
        "        for r in rows:\n",
        "            clean.append({k: (as_list(v) if k in (\"actions\", \"targets\") else short(v)) for k, v in r.items()})\n",
        "        out[\"unfair_terms\"][tt] = clean\n",
        "\n",
        "    return out\n",
        "\n",
        "def run_batch(in_glob=\"phase3/results/*.ttl\", out_dir=\"evaluation\", max_workers=6):\n",
        "    in_paths = sorted(pathlib.Path(\".\").glob(in_glob))\n",
        "    out_dir = pathlib.Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _save(res: dict):\n",
        "        p = pathlib.Path(res[\"file\"])\n",
        "        out_path = out_dir / f\"{p.stem}_results.json\"\n",
        "        out_path.write_text(json.dumps(res, ensure_ascii=False, indent=2))\n",
        "        return out_path\n",
        "\n",
        "    index = []\n",
        "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = {ex.submit(run_all_for_file, str(p)): str(p) for p in in_paths}\n",
        "        for fut in cf.as_completed(futs):\n",
        "            res = fut.result()\n",
        "            saved = _save(res)\n",
        "            index.append({\"file\": res[\"file\"], \"out\": str(saved), \"calls\": res[\"_meta\"][\"calls\"], \"ok\": res[\"_meta\"][\"ok\"]})\n",
        "            print(f\"✓ {res['file']} -> {saved.name} | {res['_meta']['ok']}/{res['_meta']['calls']} OK\")\n",
        "\n",
        "    (out_dir / \"_index.json\").write_text(json.dumps(index, ensure_ascii=False, indent=2))\n",
        "    return index\n",
        "\n",
        "run_batch(in_glob=\"phase3/results/*.ttl\", out_dir=\"evaluation\", max_workers=6)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.10.2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
